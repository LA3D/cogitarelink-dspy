# Training and Optimizing the Reflection Memory System


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Setup and Configuration

First, we’ll set up the notebook with the required imports and configure
DSPy with an appropriate LLM.

``` python
# Configure DSPy with your preferred LLM
# You can use OpenAI, Anthropic, or any local model supported by DSPy

# Example for OpenAI
# If using OpenAI, set your API key in the environment or pass it directly
# os.environ["OPENAI_API_KEY"] = "your-api-key"
# lm = dspy.OpenAI(model="gpt-4o")

# Example for Anthropic
# os.environ["ANTHROPIC_API_KEY"] = "your-api-key"
# lm = dspy.LM("anthropic/claude-3-opus-20240229")

# For testing without an actual LLM, we'll use a mock LLM
class MockLM(dspy.LM):
    def __init__(self):
        self.history = []
    
    def basic_request(self, prompt, **kwargs):
        self.history.append(prompt)
        return ["This is a mock response for testing"]

lm = MockLM()
dspy.configure(lm=lm)
```

## Create or Load Training Data

Next, we’ll load our development set for training the memory components.
In this case, we’ll use the examples from `devset_memory.jsonl`.

``` python
def load_devset(path="../tests/devset_memory.jsonl"):
    """Load the memory development set from JSONL."""
    examples = []
    
    with open(path, 'r') as f:
        for line in f:
            data = json.loads(line)
            # Convert to DSPy Example format
            example = dspy.Example(
                q=data["q"],
                exp_tool=data["exp_tool"],
                use_memory=data.get("use_memory", False)
            ).with_inputs("q")
            examples.append(example)
            
    return examples

# Load the development set
devset = load_devset()
```

``` python
# Display the development set
for i, example in enumerate(devset):
    print(f"Example {i+1}:")
    print(f"Query: {example.q}")
    print(f"Expected Tool: {example.exp_tool}")
    print(f"Use Memory: {example.use_memory}")
    print("---")
```

    Example 1:
    Query: Remember that wdt:P1476 is title
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 2:
    Query: What's the Wikidata title property?
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 3:
    Query: Inject notes into system prompt
    Expected Tool: ReflectionPrompt
    Use Memory: False
    ---
    Example 4:
    Query: Store the fact that schema:name is a common property
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 5:
    Query: Can you save rdfs:label for later reference?
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 6:
    Query: Make a note that owl:sameAs indicates identity between resources
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 7:
    Query: Remember that foaf:Person is a class for people
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 8:
    Query: Remember that dc:creator represents the author
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 9:
    Query: Store information that skos:broader indicates hierarchy
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 10:
    Query: Please recall what you know about Wikidata properties
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 11:
    Query: What did I tell you about rdfs:label?
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 12:
    Query: Can you retrieve information about owl:sameAs?
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 13:
    Query: What do you remember about FOAF ontology?
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 14:
    Query: Tell me what you know about Dublin Core properties
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 15:
    Query: What did we discuss about SKOS?
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 16:
    Query: Format your notes for the system prompt
    Expected Tool: ReflectionPrompt
    Use Memory: False
    ---
    Example 17:
    Query: Prepare relevant memory for inclusion in prompt
    Expected Tool: ReflectionPrompt
    Use Memory: False
    ---
    Example 18:
    Query: Create a memory summary for the system context
    Expected Tool: ReflectionPrompt
    Use Memory: False
    ---
    Example 19:
    Query: Inject your knowledge about RDF into the prompt
    Expected Tool: ReflectionPrompt
    Use Memory: False
    ---
    Example 20:
    Query: Format stored notes about ontologies for system use
    Expected Tool: ReflectionPrompt
    Use Memory: False
    ---
    Example 21:
    Query: Generate a memory summary for the agent
    Expected Tool: ReflectionPrompt
    Use Memory: False
    ---
    Example 22:
    Query: Add dct:title to your knowledge base
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 23:
    Query: Note that geo:lat and geo:long are for coordinates
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 24:
    Query: I want you to remember that rdf:type connects instances to classes
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 25:
    Query: Remember vocab:hasTag is for tagging resources
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 26:
    Query: Store the fact that vcard:fn is for formatted names
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 27:
    Query: Keep in mind that sioc:content contains the text content
    Expected Tool: AddReflection
    Use Memory: False
    ---
    Example 28:
    Query: What is dct:title used for?
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 29:
    Query: Tell me about geographic coordinates in RDF
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 30:
    Query: What do you recall about rdf:type?
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 31:
    Query: Can you remind me what vocab:hasTag is for?
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 32:
    Query: What is vcard:fn in the vCard ontology?
    Expected Tool: RecallReflection
    Use Memory: True
    ---
    Example 33:
    Query: What was sioc:content used for?
    Expected Tool: RecallReflection
    Use Memory: True
    ---

## Define the Metric Function

We’ll define a metric function to evaluate whether the agent is
selecting the correct tool.

------------------------------------------------------------------------

<a
href="https://github.com/LA3D/cogitarelink-dspy/blob/main/cogitarelink_dspy/memory_training.py#L44"
target="_blank" style="float:right; font-size:smaller">source</a>

### tool_match

>  tool_match (pred, sample)

*Check if the expected tool is in the trace.*

## Create a Memory Planner Agent

Now, we’ll define a DSPy Module that integrates the memory tools.

------------------------------------------------------------------------

<a
href="https://github.com/LA3D/cogitarelink-dspy/blob/main/cogitarelink_dspy/memory_training.py#L49"
target="_blank" style="float:right; font-size:smaller">source</a>

### MemoryPlanner

>  MemoryPlanner (graph_manager=None)

*A DSPy module that selects the appropriate memory operation based on
the query.*

## Train the Memory Planner with DSPy

Now we’ll train our memory planner using the DSPy BootstrapFewShot
optimizer.

------------------------------------------------------------------------

<a
href="https://github.com/LA3D/cogitarelink-dspy/blob/main/cogitarelink_dspy/memory_training.py#L103"
target="_blank" style="float:right; font-size:smaller">source</a>

### train_memory_planner

>  train_memory_planner (devset, metric=<function tool_match>,
>                            num_iterations=3, graph_manager=None)

*Train the memory planner using DSPy’s compilation framework.*

------------------------------------------------------------------------

<a
href="https://github.com/LA3D/cogitarelink-dspy/blob/main/cogitarelink_dspy/memory_training.py#L126"
target="_blank" style="float:right; font-size:smaller">source</a>

### train_memory_planner_simba

>  train_memory_planner_simba (trainset, metric=<function tool_match>,
>                                  graph_manager=None, max_steps:int=20,
>                                  max_demos:int=5, seed:int=42)

*Train the MemoryPlanner’s tool-selection policy using SIMBA.*

``` python
# Train the memory planner
from cogitarelink.core.graph import GraphManager
graph_manager = GraphManager()
if len(devset) < 32:
    # fallback to BootstrapFewShot when dataset is too small
    print(f"Trainset too small ({len(devset)} < 32); using BootstrapFewShot instead")
    optimized_planner = train_memory_planner(devset, metric=tool_match, graph_manager=graph_manager)
else:
    optimized_planner = train_memory_planner_simba(
        trainset=devset,
        graph_manager=graph_manager,
        max_steps=20,
        max_demos=5,
        seed=42,
    )
print("✅ Training complete.")
```

## Save and Load the Optimized Planner

Now we’ll save the optimized planner so it can be distributed with the
package.

------------------------------------------------------------------------

<a
href="https://github.com/LA3D/cogitarelink-dspy/blob/main/cogitarelink_dspy/memory_training.py#L162"
target="_blank" style="float:right; font-size:smaller">source</a>

### load_optimized_planner

>  load_optimized_planner
>                              (path='../cogitarelink_dspy/optimized/memory_plan
>                              ner.pkl', graph_manager=None)

*Load the optimized memory planner from disk.*

------------------------------------------------------------------------

<a
href="https://github.com/LA3D/cogitarelink-dspy/blob/main/cogitarelink_dspy/memory_training.py#L151"
target="_blank" style="float:right; font-size:smaller">source</a>

### save_optimized_planner

>  save_optimized_planner (planner,
>                              path='../cogitarelink_dspy/optimized/memory_plann
>                              er.pkl')

*Save the optimized memory planner to disk.*

``` python
# Save the optimized planner
saved_path = save_optimized_planner(optimized_planner)
print(f"Saved optimized planner to {saved_path}")
```

## Test the Optimized Planner

Let’s test the optimized planner with some examples.

``` python
# Load the optimized planner
loaded_planner = load_optimized_planner()

# Test with a few examples
test_queries = [
    "Remember that owl:sameAs is used for identity statements",
    "What do you know about Wikidata properties?",
    "Format the recent notes for inclusion in the prompt"
]

for query in test_queries:
    result = loaded_planner(q=query)
    print(f"Query: {query}")
    print(f"Selected Tool: {result['trace']}")
    print(f"Response: {result['response']}")
    print("---")
```

## Integration with the Cogitarelink DSPy Agent

Now we’ll show how to integrate the optimized memory planner with the
main Cogitarelink DSPy agent.

------------------------------------------------------------------------

<a
href="https://github.com/LA3D/cogitarelink-dspy/blob/main/cogitarelink_dspy/memory_training.py#L179"
target="_blank" style="float:right; font-size:smaller">source</a>

### CogitarelinkAgent

>  CogitarelinkAgent (graph_manager=None)

*A DSPy agent for Cogitarelink with integrated reflection memory.*

``` python
# Create the full agent
agent = CogitarelinkAgent()

# Test with different types of queries
queries = [
    "Remember that rdfs:label is used for display names",  # Memory operation
    "What is the capital of France?",  # Regular query
    "What important information do you have about Wikidata properties?"  # Query that uses memory context
]

for query in queries:
    result = agent(query=query)
    print(f"Query: {query}")
    print(f"Tool Used: {result['tool_used']}")
    print(f"Is Memory Operation: {result['is_memory_operation']}")
    print(f"Response: {result['response']}")
    print("---")
```

## Conclusion

In this notebook, we’ve demonstrated how to:

1.  Define a memory planner that integrates with DSPy
2.  Train and optimize the planner using DSPy’s compilation framework
3.  Save and load the optimized planner for distribution
4.  Integrate the memory system with a full Cogitarelink agent

This approach allows for declarative optimization of the memory system
and ensures that agents can effectively utilize semantic memory for
reflective learning.
