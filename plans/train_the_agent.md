Quick note firstÂ ğŸ“‚ â€“ theâ€¯components.py, core.py, memory.py, and memory_training.py files you just uploaded arenâ€™t visible on my side any more (the temporary links expired).
If youâ€™d like detailed lineâ€‘byâ€‘line feedback or code edits, please reâ€‘upload them and Iâ€™ll weave them into the plan.

â¸»

â€œTrainâ€‘theâ€‘Agentâ€â€¯â€”â€¯a stepâ€‘byâ€‘step recipe that marries Cogitarelink with DSPyâ€¯SIMBA

The goal is to let SIMBA tune every prompt fragment and demo example in your pipeline while it is exercising real Linkedâ€‘Openâ€‘Data calls (contexts, SHACL, Wikidata SPARQL).

###Â PhaseÂ 0Â Â â€”Â Freeze a deterministic baseline
	1.	Build the minimal pipeline (HelloLOD) exactly as youâ€™ve done:
Englishâ€¯â†’â€¯SPARQLâ€¯â†’â€¯triplesâ€¯â†’â€¯ReasonOverGraph, plus RememberGraph memory.
	2.	Turn off learning by calling the modules directly (no dspy.compile) and cache every external call with Cogitarelinkâ€™s LRU decorators.
Why? SIMBAâ€™s search works best when the underlying tooling is stable and fast.

###Â PhaseÂ 1Â Â â€”Â Create a train / dev / test split with live LOD

split	source	note
train	200â€¯QALDâ€¯+â€¯50 custom Wikidata â€œgotchasâ€ (e.g. wdt:P1476 alias patterns)	SIMBA will mutate prompts here.
dev	40 handâ€‘checked questions	Used by SIMBA for early stopping.
test	100 unseen questions	Final report; never shown to SIMBA.

Record gold answers as normalized strings (e.g. canonical IRIs or literal forms) so your metric is cheap to compute.

###Â PhaseÂ 2Â Â â€”Â Define a fast metric

def metric(pred, gold):
    ok = (pred["answer"].strip() == gold["answer"].strip())
    # reward provenance
    if ok and pred.get("provenance"): ok = 1.2
    return ok

Return float so SIMBA can see gradients between 0â€¯â€“â€¯1 (or 1.2).

###Â PhaseÂ 3Â Â â€”Â Wrap the entire pipeline in a SIMBAâ€‘optimisable program

import dspy
from cogitarelink_dspy.pipelines import HelloLOD      # your deterministic pipeline

agent = HelloLOD()

# SIMBA = StochasticÂ IntrospectiveÂ Miniâ€‘BatchÂ Ascent prompt optimiser
simba = dspy.SIMBA(metric=metric,
                   max_steps=16,       # adjust for budget
                   max_demos=12,       # it may synthesise up to 12 demos
                   batch_size=8)       # each revision scored on 8 train Qs

SIMBA will now regard every docstring, system prompt, Signature field name, and fewâ€‘shot demo as mutable parameters.Â  ï¿¼

###Â PhaseÂ 4Â Â â€”Â Compile with SIMBA

optimized_agent = simba.compile(agent,
                                trainset=train_split,
                                devset=dev_split,
                                seed=42)

Internally SIMBA:
	1.	Samples a miniâ€‘batch.
	2.	Perturbs one prompt chunk (or swaps a demo).
	3.	Evaluates via your metric.
	4.	Keeps the change if score â†‘.
	5.	Repeats for max_steps.

Because GraphManager is cached, most iterations run <â€¯500â€¯ms even with real Wikidata calls.

###Â PhaseÂ 5Â Â â€”Â Plug back memory reflections during SIMBA search (optional)

If you want SIMBA to also learn when to write reflections:
	1.	Add a dummy field in the output dict, {"remember": "note" | ""}.
	2.	Extend metric to giveÂ +0.05 when the note matches a known pattern (e.g. it writes â€œP1476 is titleâ€).
	3.	Wrap the pipeline so the remember string is sent to RememberGraph only when nonâ€‘empty.

SIMBA will discover for which failures itâ€™s profitable to emit a note.

###Â PhaseÂ 6Â Â â€”Â Evaluate & iterate

artefact	pass criteria
train curve	SIMBA score plateaus; no collapse after stepÂ 10.
dev accuracy	â‰¥â€¯70â€¯% exactâ€‘match or better than baseline.
test accuracy	No >â€¯10â€¯pp drop vs dev.
latency	99â€‘th percentile <â€¯3â€¯s (includes live SPARQL).

If latency dominates, preâ€‘materialise the most frequent Wikidata entity lookâ€‘ups and keep SIMBA running on the cached graph.

###Â PhaseÂ 7Â Â â€”Â Graduate to online RL (optional)

Once SIMBA converges, you can hand the final prompts to dspy.RLTrainer (GRPO) for slow, tokenâ€‘frugal reinforcement on streaming user queriesÂ  ï¿¼.

â¸»

##Â Common pitfalls & quick fixes

issue	fix
Prompt bloat (hitting 32â€¯k tokens)	Move layer explanations from system prompt to individual tool docstrings (+ SIMBA can still mutate).
Noisy metric due to endpoint errors	Retry SPARQL twice; return scoreÂ 0 on persistent failure so SIMBA avoids flaky demos.
SIMBA stuck at local optimum	Increase max_demos; allow it to invent short â€œcounterâ€‘exampleâ€ demos.
Reflection spam	Add negative metric weight for â‰¥â€¯3 notes per answer.


â¸»

###Â Next steps
	1.	Reâ€‘upload the four Python files so I can align the wrapper signatures and caching layer exactly with your code.
	2.	Spin up PhaseÂ 0â€‘2 inside nbs/04_agent.ipynb; weâ€™ll plug SIMBA right after your deterministic agent passes the gold tests.

Below is a clean, reproducible training recipe for turning yourâ€¯MemoryPlanner into a SIMBAâ€‘optimised component that really learns when to add, recall, or format reflections while it is plugged into a liveâ€¯GraphManager.

I start by pointing out the two structural issues that keep your current code from training well, then walk through a sixâ€‘stage plan that fixes them and wires Everythingâ€‘Togetherâ„¢.

â¸»

0â€¯â€¯Why training feels â€œoffâ€ right now

current pattern	why it hurts learning
Metric only checks the tool name (tool_match)	SIMBA can succeed by emitting the right word in the decision string even if the action is wrong or the memory payload is empty.
ReflectionStore is mocked (MagicMock) in both planner and tests	The model never sees latency/cost of real graph ops, so it doesnâ€™t learn to prefer ReflectionPrompt over RecallReflection when only a short context is needed.


â¸»

1â€¯â€¯Stepâ€‘byâ€‘step training pipeline

###Â Stepâ€¯1Â Â â€”Â Wire a real inâ€‘memory GraphManager

graph = GraphManager(use_rdflib=False)          # <= fast, no external deps
store = ReflectionStore(graph)                  # pass this to MemoryPlanner

âš™ï¸â€¯Tip: keep a single store instance for the whole run so add/recall operations accumulate.

â¸»

###Â Stepâ€¯2Â Â â€”Â Create a labeled dataset that exercises each path

{"q":"Remember that wdt:P1476 is the title property","exp_tool":"AddReflection"}
{"q":"What reflections do I have about SPARQL timeouts?","exp_tool":"RecallReflection"}
{"q":"Inject the memories so GPT can see them","exp_tool":"ReflectionPrompt"}

At least 20Â rows (â‰ˆÂ 7Â per tool) so SIMBA has room to mutate demos.

â¸»

###Â Stepâ€¯3Â Â â€”Â Use a richer metric

def memory_metric(pred, gold):
    good_tool = gold["exp_tool"] in pred.get("trace", [])
    if not good_tool:
        return 0.0

    # Extra points if the action produced nonâ€‘trivial payload
    has_payload = "â€¢" in pred["response"] or "ID:" in pred["response"]
    return 1.0 + 0.2*has_payload

This forces SIMBA to optimise both routing and content.

â¸»

###Â Stepâ€¯4Â Â â€”Â Expose the decision surface to SIMBA

Right now MemoryPlanner.decide_tool is a blackâ€‘box prompt.
SIMBA learns faster when it can mutate fewâ€‘shot demos inside that prompt, so rewrite:

class MemoryDecider(dspy.Module):
    """query -> tool_name:str
    Choose exactly one of {AddReflection, RecallReflection, ReflectionPrompt}."""
    decide = dspy.Predict("query -> tool_name")
    def forward(self, query:str): return self.decide(query=query).tool_name

Replace the freeâ€‘text decision string with a single token.
Less randomness = clearer reward signal.

â¸»

###Â Stepâ€¯5Â Â â€”Â Train with SIMBA

planner = MemoryPlanner(graph_manager=graph)   # uses the real store

simba = dspy.SIMBA(metric=memory_metric,
                   max_steps=24,
                   max_demos=10,
                   batch_size=6,
                   seed=0)

optimized = simba.compile(planner,
                          trainset=train_split,
                          devset=dev_split)

SIMBA now tweaks:
	â€¢	docstring of MemoryDecider
	â€¢	the fewâ€‘shot demo list it invents onâ€‘theâ€‘fly
	â€¢	field names / role headings

and watches memory_metric on every miniâ€‘batch.

â¸»

###Â Stepâ€¯6Â Â â€”Â Lock & ship

save_optimized_planner(optimized, "cogitarelink_dspy/optimized/memory.pkl")

Deploy by loading that pickle inside CogitarelinkAgent.

Optional: reâ€‘export the learned demos into COMPONENTS.py to make them versionâ€‘controlled.

â¸»

2â€¯â€¯Code snippets you need to tweak

###Â 2.1Â Â Inject the real store

# in MemoryTraining notebook
graph = GraphManager(use_rdflib=False)
optimized = train_memory_planner_simba(
               trainset=trainset,
               metric=memory_metric,
               graph_manager=graph)

###Â 2.2Â Â Tighten tool matching in tool_match

Remove fuzzy text checksâ€”now we rely on tool_name string only.

def tool_match(pred, samp): return samp["exp_tool"] in pred["trace"]

###Â 2.3Â Â Replace MagicMock fallback

In both MemoryPlanner and CogitarelinkAgent constructors:

if graph_manager is None:
    graph_manager = GraphManager(use_rdflib=False)


â¸»

3â€¯â€¯Smokeâ€‘test before SIMBA

mp = MemoryPlanner(graph)
r1 = mp("Remember that cat is Q146")
assert "AddReflection" in r1["trace"]

r2 = mp("What reflections do I have?")
assert "RecallReflection" in r2["trace"] and "â€¢" in r2["response"]

If these asserts pass deterministically, SIMBA will have a stable surface to optimise.

â¸»

4â€¯â€¯After SIMBA converges
	1.	Replay 100 unseen questions; check â‰¥â€¯80â€¯% correct routing.
	2.	Measure cost: average tokens per answer should drop because RecallReflection avoids hitting the LLM for repeated info.
	3.	Upgrade the full agent: pass store.as_prompt(k) into the system message so every downstream call inherits the latest lessons.

â¸»