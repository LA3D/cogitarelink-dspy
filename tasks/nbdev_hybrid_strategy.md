<!---
  Hybrid nbdev + Code Module Strategy
-->
# Hybrid nbdev & Standalone Modules

This document outlines when to use nbdev notebooks versus pure Python modules, to balance exploratory development with a clean, Codex-friendly codebase.

## 1. nbdev as Source of Truth
- Keep all narrative, diagrams, and exploratory code in `nbs/` notebooks.
- Export shared code into `cogitarelink_dspy/` via nbdev: each exported `.py` file gets a header linking back to its notebook (e.g. `# AUTOGENERATED! File to edit: ../nbs/...ipynb`).
- **Rule**: never manually edit an exported file; changes belong in the originating notebook.

## 2. Standalone .py Modules
- Any `.py` without the nbdev autogenerated header is standalone, hand-maintained code.
- Ideal for:
  - Core classes & data structures (ReflectionStore, TelemetryStore)
  - COMPONENTS registry and wrapper generator
  - Utility scripts (`scripts/`, e.g. `generate_batch.py`, `check_curriculum.py`)
  - Metrics and test harnesses (`metrics.py`, `tests/test_*.py`)
- These files load and run without notebook overhead—perfect for Codex code completion, CI pipelines, and reuse.

## 3. What Goes in Notebooks
- Literate programming: explain concepts, show diagrams, include markdown annotations.
- Exploratory pipelines: prompt engineering, few-shot demos, SIMBA tuning loops.
- Data visualizations: charts, evaluation reports, telemetry dashboards.
- Quick prototyping: test a new wrapper, inspect raw graph triples, debug SPARQL queries.

## 4. Workflow
1. Prototype new code in a notebook cell; verify interactively.
2. Once stable, move implementation to a dedicated `.py` module:
   - Add code in `nbs/*.ipynb`, export via nbdev, which updates `cogitarelink_dspy/`.
   - Or write directly in a standalone `.py` if intended as a utility or script.
3. Import the module back into notebooks for further usage.

## 5. CI Guardrails
- Enforce pre-commit or CI check:
  - Do not allow manual edits in nbdev-generated `.py` files unless the source notebook is updated.
  - Validate existence of nbdev header in files exported from notebooks.
- Ensure standalone modules lack the nbdev header, confirming they are safe to edit.

## 6. Benefits
- **Exploration**: notebooks for rapid experimentation, rich documentation.
- **Stability**: clean modules for core logic and reproducibility.
- **Codex integration**: pure `.py` files simplify model-based refactoring and code synthesis.
- **Versioning**: nbdev export keeps code and docs in sync, with clear provenance from notebooks.

By following this hybrid approach, you get the best of both worlds: an interactive, literate R&D environment plus a robust, maintainable codebase.

## 7. Notebook Refactor & Creation Plan
Below is the high-level notebook re-organization and new notebook scaffold to align with our hybrid strategy:

1. Refactor Existing Notebooks
   - **nbs/00_core.ipynb**
     * Keep LM setup and minimal `Echo` demo labeled “playground.”
     * Remove detailed tool calls; focus on core configuration.
   - **nbs/01_components.ipynb** → **nbs/01_components_update.ipynb**
     * Load and validate the expanded COMPONENTS registry (memory, telemetry, doc-ingest, GC tools).
     * Display tools grouped by layer.
   - **nbs/02_wrappers.ipynb**
     * Regenerate DSPy wrappers from the updated registry.
     * Smoke-test each new tool (`FetchDoc`, `LogTelemetry`, `SummarizeReflections`).
   - **nbs/03_memory.ipynb**
     * Demonstrate `ReflectionStore` using `@set`/`@container` indexing and framing for O(1) retrieval.
   - **nbs/04_memory_training.ipynb**
     * Deprecate or archive: merge few-shot/SIMBA routines into the curriculum training notebook.

2. Create New Notebooks
   - **nbs/00_memory_telemetry.ipynb**
     * Implement and test `ReflectionStore` + `TelemetryStore` with context and provenance.
   - **nbs/03_pipelines.ipynb**
     * Assemble `HelloLOD`, `FullPlanner`, `HelloLODWithMemory` with injected stores.
     * Run end-to-end examples per layer.
   - **nbs/04_curriculum_authoring.ipynb**
     * Integrate `generate_batch.py` helper for stages 00–12.
     * Codex-assisted draft → append → smoke-test workflow.
   - **nbs/05_curriculum_training.ipynb**
     * Load train/dev/test splits.
     * Run `BootstrapFewShot` then `SIMBA.compile`, track metrics.
   - **nbs/06_evaluation.ipynb**
     * Evaluate `FullAgent` on test split, compute provenance bonuses and latency percentiles.
   - **nbs/07_reflection_gc.ipynb**
     * Demonstrate `SummarizeReflections` tool on a large memory graph.
   - **nbs/08_doc_ingestion_demo.ipynb**
     * Use `FetchDoc` + `IngestDoc` to ingest JSON-LD spec, then query ingested triples.
   - **nbs/09_telemetry_dashboard.ipynb** *(optional)*
     * Query `urn:agent:telemetry` graph and render latency, cache-hit, error charts.

3. Standalone Modules & CI Scripts
   - Keep utility scripts in `scripts/` (e.g. `generate_batch.py`, `check_curriculum.py`, `triage_ui.py`).
   - Maintain `metrics.py` and `tests/*.py` as pure modules for Codex-friendly editing.
   - Enforce via CI that nbdev-exported `.py` files are not manually edited without updating the source notebook.