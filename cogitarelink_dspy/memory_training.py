"""A guide to training and saving the Reflection Memory component for Cogitarelink DSPy agents."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_memory_training.ipynb.

# %% auto 0
__all__ = ['devset', 'load_devset', 'tool_match', 'MemoryPlanner', 'train_memory_planner', 'train_memory_planner_simba',
           'save_optimized_planner', 'load_optimized_planner', 'CogitarelinkAgent']

# %% ../nbs/04_memory_training.ipynb 5
import os
import json
import pickle
import dspy
from dspy.teleprompt import BootstrapFewShot, MIPROv2
from pathlib import Path
from unittest.mock import MagicMock

from .wrappers import get_tool_by_name
from .memory import ReflectionStore
from cogitarelink.core.graph import GraphManager

# %% ../nbs/04_memory_training.ipynb 8
def load_devset(path="../tests/devset_memory.jsonl"):
    """Load the memory development set from JSONL."""
    examples = []
    
    with open(path, 'r') as f:
        for line in f:
            data = json.loads(line)
            # Convert to DSPy Example format
            example = dspy.Example(
                q=data["q"],
                exp_tool=data["exp_tool"],
                use_memory=data.get("use_memory", False)
            ).with_inputs("q")
            examples.append(example)
            
    return examples

# Load the development set
devset = load_devset()

# %% ../nbs/04_memory_training.ipynb 11
def tool_match(pred, sample):
    """Check if the expected tool is in the trace."""
    return sample["exp_tool"] in pred.get("trace", [])

# %% ../nbs/04_memory_training.ipynb 13
class MemoryPlanner(dspy.Module):
    """A DSPy module that selects the appropriate memory operation based on the query."""
    
    def __init__(self, graph_manager=None):
        super().__init__()
        
        # If no graph manager is provided, create a mock one for testing
        if graph_manager is None:
            graph_manager = MagicMock(spec=GraphManager)
        
        # Create the reflection store
        self.reflection_store = ReflectionStore(graph_manager)
        
        # Get all memory tools
        self.add_reflection = get_tool_by_name("AddReflection")()
        self.recall_reflection = get_tool_by_name("RecallReflection")()
        self.reflection_prompt = get_tool_by_name("ReflectionPrompt")()
        
        # Define the Chain of Thought for deciding which tool to use
        self.decide_tool = dspy.ChainOfThought("query -> decision: str")
        
    def forward(self, q):
        """Process a query and decide which memory tool to use."""
        trace = []
        
        # Use ChainOfThought to decide which tool to use
        tool_decision = self.decide_tool(query=q)
        
        # Based on the decision, choose the appropriate tool
        if "add" in tool_decision.decision.lower() or "store" in tool_decision.decision.lower() or "remember" in tool_decision.decision.lower():
            note_id = self.add_reflection(text=q, tags=["user_query"])
            trace.append("AddReflection")
            response = f"I've stored that information with ID: {note_id}"
            
        elif "recall" in tool_decision.decision.lower() or "retrieve" in tool_decision.decision.lower() or "what" in tool_decision.decision.lower():
            notes = self.recall_reflection(limit=3)
            trace.append("RecallReflection")
            if notes:
                notes_text = "\n".join([f"• {note.content['text']}" for note in notes])
                response = f"Here's what I recall:\n{notes_text}"
            else:
                response = "I don't have any relevant memories about that."
                
        elif "prompt" in tool_decision.decision.lower() or "format" in tool_decision.decision.lower() or "inject" in tool_decision.decision.lower():
            formatted = self.reflection_prompt(limit=5)
            trace.append("ReflectionPrompt")
            response = f"I've prepared these notes for inclusion in the prompt:\n{formatted}"
            
        else:
            response = "I'm not sure how to process that as a memory operation."
            
        return {"response": response, "trace": trace, "tool_decision": tool_decision.decision}

# %% ../nbs/04_memory_training.ipynb 15
def train_memory_planner(devset, metric=tool_match, num_iterations=3, graph_manager=None):
    """Train the memory planner using DSPy's compilation framework."""
    # Create the base planner
    planner = MemoryPlanner(graph_manager)
    
    # Set up the bootstrap trainer
    trainer = BootstrapFewShot(trainset=devset, metric=metric)
    
    # Configure search space for optimization
    search_space = {
        "RecallReflection.limit": [3, 5, 10],
        "ReflectionPrompt.limit": [3, 5, 10]
    }
    
    # When using a real LLM, uncomment this line to run compilation
    # optimized_planner = dspy.compile(planner, trainer, num_iterations=num_iterations, search_space=search_space)
    
    # For testing without a real LLM, we'll just return the unoptimized planner
    optimized_planner = planner
    
    return optimized_planner

# %% ../nbs/04_memory_training.ipynb 16
def train_memory_planner_simba(
    trainset,
    metric=tool_match,
    graph_manager=None,
    max_steps: int = 20,
    max_demos: int = 5,
    seed: int = 42,
):
    """Train the MemoryPlanner’s tool-selection policy using SIMBA."""
    planner = MemoryPlanner(graph_manager)
    import dspy
    simba = dspy.SIMBA(
        metric=metric,
        max_steps=max_steps,
        max_demos=max_demos,
    )
    optimized = simba.compile(
        student=planner,
        trainset=trainset,
        seed=seed,
    )
    return optimized


# %% ../nbs/04_memory_training.ipynb 19
def save_optimized_planner(planner, path="../cogitarelink_dspy/optimized/memory_planner.pkl"):
    """Save the optimized memory planner to disk."""
    # Ensure the directory exists
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    # Save the planner using pickle
    with open(path, 'wb') as f:
        pickle.dump(planner, f)
    
    return path

def load_optimized_planner(path="../cogitarelink_dspy/optimized/memory_planner.pkl", graph_manager=None):
    """Load the optimized memory planner from disk."""
    # Check if the file exists
    if not os.path.exists(path):
        raise FileNotFoundError(f"Optimized planner not found at {path}")
    
    # Load the planner using pickle
    with open(path, 'rb') as f:
        planner = pickle.load(f)
    
    # If a graph manager is provided, update the planner's reflection store
    if graph_manager is not None:
        planner.reflection_store = ReflectionStore(graph_manager)
    
    return planner

# %% ../nbs/04_memory_training.ipynb 24
class CogitarelinkAgent(dspy.Module):
    """A DSPy agent for Cogitarelink with integrated reflection memory."""
    
    def __init__(self, graph_manager=None):
        super().__init__()
        
        # If no graph manager is provided, create a mock one for testing
        if graph_manager is None:
            graph_manager = MagicMock(spec=GraphManager)
        
        # Load the optimized memory planner
        try:
            self.memory_planner = load_optimized_planner(graph_manager=graph_manager)
        except FileNotFoundError:
            # Fall back to unoptimized planner if optimized version not found
            self.memory_planner = MemoryPlanner(graph_manager)
        
        # Main agent's Chain of Thought
        self.main_cot = dspy.ChainOfThought("query, context -> response")
        
        # Tool selection module
        self.select_tool = dspy.ChainOfThought("query -> tool_name: str, is_memory: bool")
        
    def forward(self, query):
        """Process a query using the appropriate tools and memory."""
        # Decide whether to use memory tools or other tools
        tool_selection = self.select_tool(query=query)
        
        if tool_selection.is_memory:
            # Use the memory planner for memory operations
            memory_result = self.memory_planner(q=query)
            return {
                "response": memory_result["response"],
                "tool_used": memory_result["trace"][0] if memory_result["trace"] else "None",
                "is_memory_operation": True
            }
        else:
            # For non-memory operations, use other tools...
            # Get recent memories as context
            context = self.memory_planner.reflection_prompt(limit=3)
            
            # Use main Chain of Thought with memory context
            result = self.main_cot(query=query, context=context)
            
            # Optionally store the interaction in memory
            if "important information" in query.lower():
                self.memory_planner.add_reflection(
                    text=f"User asked: {query} | Response: {result.response}",
                    tags=["interaction"]
                )
            
            return {
                "response": result.response,
                "tool_used": tool_selection.tool_name,
                "is_memory_operation": False
            }
