{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello-World DSPy Agent\n",
    "\n",
    "> A minimal DSPy agent implementation to echo or compute simple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nimport dspy\nimport os\n\n# Configure a default LLM - using gpt-4o-mini as specified in the plan\n# Note: In real usage, you'd set OPENAI_API_KEY in your environment\ndefault_lm = dspy.LM(\"openai/gpt-4o-mini\")\ndspy.configure(lm=default_lm)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nfrom typing import Dict, Any, List, Optional\nfrom cogitarelink_dspy.wrappers import get_tools, group_tools_by_layer\n\ndef make_hello_agent(llm=None):\n    \"\"\"Create a simple DSPy agent with dynamic tool loading and LLM capabilities.\n    \n    Args:\n        llm: A DSPy language model. If None, uses the default configured LM.\n    \n    Returns:\n        A HelloAgent instance that can process messages using all available tools and LLM.\n    \"\"\"\n    # Use provided LLM or the default one\n    lm = llm or dspy.settings.lm\n    \n    # Create a simple signature for LLM-based response generation\n    signature = dspy.Signature(\n        \"message -> response\",\n        \"Generate a friendly response to the input message.\"\n    )\n    \n    # Create an Agent class that can use all tools and LLM\n    class HelloAgent(dspy.Module):\n        \"\"\"A Semantic Web agent that understands layer architecture and uses the appropriate tools.\n        \n        This agent dynamically loads all available tools from the component registry\n        and organizes them by semantic layer.\n        \"\"\"\n        # Define signature with instructions to avoid DSPy error\n        signature = dspy.Signature(\n            \"message -> echo_result, llm_response, layer_used\",\n            \"Generate a response using tools and identify the semantic layer.\"\n        )\n        \n        def __init__(self):\n            super().__init__()\n            # Initialize predictor for LLM responses\n            self.predictor = dspy.ChainOfThought(signature)\n            \n            try:\n                # Try to load tools, but have a fallback for testing\n                self.tools = get_tools()\n                self.tools_by_layer = group_tools_by_layer(self.tools)\n            except Exception as e:\n                # Fallback for notebook testing\n                print(f\"Warning: Failed to load tools: {e}\")\n                self.tools = []\n                self.tools_by_layer = {\"Utility\": []}\n            \n        def forward(self, message: str) -> Dict[str, Any]:\n            \"\"\"Process a message using tools and LLM.\n            \n            Args:\n                message: The input message to process\n                \n            Returns:\n                Dict with echo_result, llm_response, and other fields\n            \"\"\"\n            # First, try to match the message to a semantic layer\n            # For simplicity here, we're just using a simple keyword-based approach\n            # In a real system, this would be a more sophisticated matching system\n            layer_used = \"Utility\"  # Default layer\n            layer_keywords = {\n                \"context\": \"Context\",\n                \"json-ld\": \"Context\",\n                \"namespace\": \"Context\",\n                \"ontology\": \"Ontology\",\n                \"vocabulary\": \"Ontology\",\n                \"rule\": \"Rules\",\n                \"validate\": \"Rules\",\n                \"triple\": \"Instances\",\n                \"graph\": \"Instances\",\n                \"signature\": \"Verification\",\n                \"verify\": \"Verification\"\n            }\n            \n            message_lower = message.lower()\n            for keyword, layer in layer_keywords.items():\n                if keyword in message_lower:\n                    layer_used = layer\n                    break\n            \n            # Get tools for the identified layer\n            layer_tools = self.tools_by_layer.get(layer_used, [])\n            \n            # For demo purposes, we'll just use the first tool in the layer if available\n            # In a real system, this would be a more intelligent tool selection\n            tool_result = \"No tool available for this layer\"\n            tool_used = \"None\"\n            \n            if layer_tools:\n                # Get the first tool and instantiate it\n                tool_class = layer_tools[0]\n                tool = tool_class()\n                tool_used = tool_class.__name__\n                \n                # For demo purposes, we just acknowledge we would call the tool\n                # In a real system with actual implementations, we would call the tool\n                tool_result = f\"Would call {tool_used} from layer {layer_used}\"\n            \n            # Get LLM response\n            llm_result = self.predictor(message=message)\n            \n            # Return the results\n            return {\n                \"echo_result\": message,  # Simple echo\n                \"llm_response\": llm_result.response,\n                \"layer_used\": layer_used,\n                \"tool_used\": tool_used,\n                \"tool_result\": tool_result,\n                \"reasoning\": getattr(llm_result, \"reasoning\", None)\n            }\n            \n    # Return a new instance of the HelloAgent\n    return HelloAgent()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Demo the Hello-World agent with dynamic tool loading capabilities\ntry:\n    agent = make_hello_agent()\n    \n    # Test with a simple message\n    message = \"Hello, DSPy! What can you do with Semantic Web data?\"\n    print(f\"Testing with message: \\\"{message}\\\"\")\n    result = agent(message)\n    \n    print(f\"\\nLayer identified: {result.get('layer_used', 'Unknown')}\")\n    print(f\"Tool selected: {result.get('tool_used', 'None')}\")\n    print(f\"\\nLLM response: {result.get('llm_response', 'No response')}\")\nexcept Exception as e:\n    print(f\"Error testing agent: {e}\")\n    print(\"This is expected during notebook testing without a real LLM connection.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}