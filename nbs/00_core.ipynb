{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78f9440",
   "metadata": {},
   "source": [
    "# Hello-World DSPy Agent\n",
    "\n",
    "> A minimal DSPy agent implementation to echo or compute simple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd26026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02310a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import dspy\n",
    "import os\n",
    "\n",
    "# Configure a default LLM - using gpt-4o-mini as specified in the plan\n",
    "# Note: In real usage, you'd set OPENAI_API_KEY in your environment\n",
    "default_lm = dspy.LM(\"openai/gpt-4o-mini\")\n",
    "dspy.configure(lm=default_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Echo(dspy.Module):\n",
    "    \"\"\"Echoes the input message back.\"\"\"\n",
    "    signature = dspy.Signature(\n",
    "        \"message -> str\",\n",
    "        \"Echoes the input message back.\"\n",
    "    )\n",
    "    def forward(self, message: str) -> str:\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c066e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Dict, Any, List, Optional\n",
    "from cogitarelink_dspy.wrappers import get_tools, group_tools_by_layer\n",
    "\n",
    "def make_hello_agent(llm=None):\n",
    "    \"\"\"Create a simple DSPy agent with dynamic tool loading and LLM capabilities.\n",
    "    \n",
    "    Args:\n",
    "        llm: A DSPy language model. If None, uses the default configured LM.\n",
    "    \n",
    "    Returns:\n",
    "        A HelloAgent instance that can process messages using all available tools and LLM.\n",
    "    \"\"\"\n",
    "    # Use provided LLM or the default one\n",
    "    lm = llm or dspy.settings.lm\n",
    "    \n",
    "    # Create a simple signature for LLM-based response generation\n",
    "    signature = dspy.Signature(\n",
    "        \"message -> response\",\n",
    "        \"Generate a friendly response to the input message.\"\n",
    "    )\n",
    "    \n",
    "    # Create an Agent class that can use all tools and LLM\n",
    "    class HelloAgent(dspy.Module):\n",
    "        \"\"\"A Semantic Web agent that understands layer architecture and uses the appropriate tools.\n",
    "        \n",
    "        This agent dynamically loads all available tools from the component registry\n",
    "        and organizes them by semantic layer.\n",
    "        \"\"\"\n",
    "        # Define signature with instructions to avoid DSPy error\n",
    "        signature = dspy.Signature(\n",
    "            \"message -> echo_result, llm_response, layer_used\",\n",
    "            \"Generate a response using tools and identify the semantic layer.\"\n",
    "        )\n",
    "        \n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            # Initialize predictor for LLM responses\n",
    "            self.predictor = dspy.ChainOfThought(signature)\n",
    "            \n",
    "            try:\n",
    "                # Try to load tools, but have a fallback for testing\n",
    "                self.tools = get_tools()\n",
    "                self.tools_by_layer = group_tools_by_layer(self.tools)\n",
    "            except Exception as e:\n",
    "                # Fallback for notebook testing\n",
    "                print(f\"Warning: Failed to load tools: {e}\")\n",
    "                self.tools = []\n",
    "                self.tools_by_layer = {\"Utility\": []}\n",
    "            \n",
    "        def forward(self, message: str) -> Dict[str, Any]:\n",
    "            \"\"\"Process a message using tools and LLM.\n",
    "            \n",
    "            Args:\n",
    "                message: The input message to process\n",
    "                \n",
    "            Returns:\n",
    "                Dict with echo_result, llm_response, and other fields\n",
    "            \"\"\"\n",
    "            # First, try to match the message to a semantic layer\n",
    "            # For simplicity here, we're just using a simple keyword-based approach\n",
    "            # In a real system, this would be a more sophisticated matching system\n",
    "            layer_used = \"Utility\"  # Default layer\n",
    "            layer_keywords = {\n",
    "                \"context\": \"Context\",\n",
    "                \"json-ld\": \"Context\",\n",
    "                \"namespace\": \"Context\",\n",
    "                \"ontology\": \"Ontology\",\n",
    "                \"vocabulary\": \"Ontology\",\n",
    "                \"rule\": \"Rules\",\n",
    "                \"validate\": \"Rules\",\n",
    "                \"triple\": \"Instances\",\n",
    "                \"instances\": \"Instances\",\n",
    "                \"graph\": \"Instances\",\n",
    "                \"signature\": \"Verification\",\n",
    "                \"verify\": \"Verification\",\n",
    "                # Catch noun form as well\n",
    "                \"verification\": \"Verification\"\n",
    "            }\n",
    "            \n",
    "            message_lower = message.lower()\n",
    "            for keyword, layer in layer_keywords.items():\n",
    "                if keyword in message_lower:\n",
    "                    layer_used = layer\n",
    "                    break\n",
    "            \n",
    "            # Get tools for the identified layer\n",
    "            layer_tools = self.tools_by_layer.get(layer_used, [])\n",
    "            \n",
    "            # For demo purposes, we'll just use the first tool in the layer if available\n",
    "            # In a real system, this would be a more intelligent tool selection\n",
    "            tool_result = \"No tool available for this layer\"\n",
    "            tool_used = \"None\"\n",
    "            \n",
    "            if layer_tools:\n",
    "                # Get the first tool and instantiate it\n",
    "                tool_class = layer_tools[0]\n",
    "                tool = tool_class()\n",
    "                tool_used = tool_class.__name__\n",
    "                \n",
    "                # For demo purposes, we just acknowledge we would call the tool\n",
    "                # In a real system with actual implementations, we would call the tool\n",
    "                tool_result = f\"Would call {tool_used} from layer {layer_used}\"\n",
    "            \n",
    "            # Get LLM response\n",
    "            llm_result = self.predictor(message=message)\n",
    "            \n",
    "            # Return the results\n",
    "            return {\n",
    "                \"echo_result\": message,  # Simple echo\n",
    "                \"llm_response\": llm_result.response,\n",
    "                \"layer_used\": layer_used,\n",
    "                \"tool_used\": tool_used,\n",
    "                \"tool_result\": tool_result,\n",
    "                \"reasoning\": getattr(llm_result, \"reasoning\", None)\n",
    "            }\n",
    "            \n",
    "    # Return a new instance of the HelloAgent\n",
    "    return HelloAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo the Hello-World agent with dynamic tool loading capabilities\n",
    "try:\n",
    "    agent = make_hello_agent()\n",
    "    \n",
    "    # Test with a simple message\n",
    "    message = \"Hello, DSPy! What can you do with Semantic Web data?\"\n",
    "    print(f\"Testing with message: \\\"{message}\\\"\")\n",
    "    result = agent(message)\n",
    "    \n",
    "    print(f\"\\nLayer identified: {result.get('layer_used', 'Unknown')}\")\n",
    "    print(f\"Tool selected: {result.get('tool_used', 'None')}\")\n",
    "    print(f\"\\nLLM response: {result.get('llm_response', 'No response')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing agent: {e}\")\n",
    "    print(\"This is expected during notebook testing without a real LLM connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
