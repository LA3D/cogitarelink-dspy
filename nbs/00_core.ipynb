{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello-World DSPy Agent\n",
    "\n",
    "> A minimal DSPy agent implementation to echo or compute simple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nimport dspy\nimport os\n\n# Configure a default LLM - using gpt-4o-mini as specified in the plan\n# Note: In real usage, you'd set OPENAI_API_KEY in your environment\ndefault_lm = dspy.LM(\"openai/gpt-4o-mini\")\ndspy.configure(lm=default_lm)\n\nclass Echo(dspy.Module):\n    \"\"\"Echoes the input message back.\"\"\"\n    def forward(self, message: str) -> str:\n        return message"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef make_hello_agent(llm=None):\n    \"\"\"Create a simple DSPy agent with the Echo tool and LLM capabilities.\n    \n    Args:\n        llm: A DSPy language model. If None, uses the default configured LM.\n    \n    Returns:\n        A HelloAgent instance that can process messages using the Echo tool and LLM.\n    \"\"\"\n    # Use provided LLM or the default one\n    lm = llm or dspy.settings.lm\n    \n    # Create a simple signature for LLM-based response generation\n    signature = dspy.Signature(\n        \"message -> response\",\n        \"Generate a friendly response to the input message.\"\n    )\n    \n    # Create an Agent class that can use Echo tool and LLM\n    class HelloAgent(dspy.Module):\n        def __init__(self):\n            super().__init__()\n            self.tools = [Echo()]\n            # Use ChainOfThought module to get LLM reasoning\n            self.predictor = dspy.ChainOfThought(signature)\n            \n        def forward(self, message):\n            # First use the Echo tool\n            echo_result = self.tools[0].forward(message)\n            \n            # Then get an LLM response\n            llm_result = self.predictor(message=message)\n            \n            # Return both results\n            return {\n                \"echo_result\": echo_result,\n                \"llm_response\": llm_result.response,\n                \"reasoning\": getattr(llm_result, \"reasoning\", None)\n            }\n            \n    # Return a new instance of the HelloAgent\n    return HelloAgent()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Demo the Hello-World agent with LLM capabilities\nagent = make_hello_agent()\nmessage = \"Hello, DSPy! What can you do with Semantic Web data?\"\nresult = agent(message)\n\nprint(\"Echo result:\", result[\"echo_result\"])\nprint(\"\\nLLM response:\", result[\"llm_response\"])\nprint(\"\\nLLM reasoning:\", result[\"reasoning\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}