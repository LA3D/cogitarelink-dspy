{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp memory_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Optimizing the Reflection Memory System\n",
    "\n",
    "> A guide to training and saving the Reflection Memory component for Cogitarelink DSPy agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/bin/bash: line 1: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Install required packages if not already installed\n",
    "!pip install -q dspy-ai torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we'll set up the notebook with the required imports and configure DSPy with an appropriate LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import dspy\n",
    "from dspy.teleprompt import BootstrapFewShot, MIPROv2\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "from cogitarelink_dspy.wrappers import get_tool_by_name\n",
    "from cogitarelink_dspy.memory import ReflectionStore\n",
    "from cogitarelink.core.graph import GraphManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DSPy with your preferred LLM\n",
    "# You can use OpenAI, Anthropic, or any local model supported by DSPy\n",
    "\n",
    "# Example for OpenAI\n",
    "# If using OpenAI, set your API key in the environment or pass it directly\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "# lm = dspy.OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Example for Anthropic\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n",
    "# lm = dspy.LM(\"anthropic/claude-3-opus-20240229\")\n",
    "\n",
    "# For testing without an actual LLM, we'll use a mock LLM\n",
    "class MockLM(dspy.LM):\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "    \n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        self.history.append(prompt)\n",
    "        return [\"This is a mock response for testing\"]\n",
    "\n",
    "lm = MockLM()\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load Training Data\n",
    "\n",
    "Next, we'll load our development set for training the memory components. In this case, we'll use the examples from `devset_memory.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_devset(path=\"../tests/devset_memory.jsonl\"):\n",
    "    \"\"\"Load the memory development set from JSONL.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # Convert to DSPy Example format\n",
    "            example = dspy.Example(\n",
    "                q=data[\"q\"],\n",
    "                exp_tool=data[\"exp_tool\"],\n",
    "                use_memory=data.get(\"use_memory\", False)\n",
    "            ).with_inputs(\"q\")\n",
    "            examples.append(example)\n",
    "            \n",
    "    return examples\n",
    "\n",
    "# Load the development set\n",
    "devset = load_devset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Query: Remember that wdt:P1476 is title\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 2:\n",
      "Query: What's the Wikidata title property?\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 3:\n",
      "Query: Inject notes into system prompt\n",
      "Expected Tool: ReflectionPrompt\n",
      "Use Memory: False\n",
      "---\n",
      "Example 4:\n",
      "Query: Store the fact that schema:name is a common property\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 5:\n",
      "Query: Can you save rdfs:label for later reference?\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 6:\n",
      "Query: Make a note that owl:sameAs indicates identity between resources\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 7:\n",
      "Query: Remember that foaf:Person is a class for people\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 8:\n",
      "Query: Remember that dc:creator represents the author\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 9:\n",
      "Query: Store information that skos:broader indicates hierarchy\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 10:\n",
      "Query: Please recall what you know about Wikidata properties\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 11:\n",
      "Query: What did I tell you about rdfs:label?\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 12:\n",
      "Query: Can you retrieve information about owl:sameAs?\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 13:\n",
      "Query: What do you remember about FOAF ontology?\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 14:\n",
      "Query: Tell me what you know about Dublin Core properties\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 15:\n",
      "Query: What did we discuss about SKOS?\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 16:\n",
      "Query: Format your notes for the system prompt\n",
      "Expected Tool: ReflectionPrompt\n",
      "Use Memory: False\n",
      "---\n",
      "Example 17:\n",
      "Query: Prepare relevant memory for inclusion in prompt\n",
      "Expected Tool: ReflectionPrompt\n",
      "Use Memory: False\n",
      "---\n",
      "Example 18:\n",
      "Query: Create a memory summary for the system context\n",
      "Expected Tool: ReflectionPrompt\n",
      "Use Memory: False\n",
      "---\n",
      "Example 19:\n",
      "Query: Inject your knowledge about RDF into the prompt\n",
      "Expected Tool: ReflectionPrompt\n",
      "Use Memory: False\n",
      "---\n",
      "Example 20:\n",
      "Query: Format stored notes about ontologies for system use\n",
      "Expected Tool: ReflectionPrompt\n",
      "Use Memory: False\n",
      "---\n",
      "Example 21:\n",
      "Query: Generate a memory summary for the agent\n",
      "Expected Tool: ReflectionPrompt\n",
      "Use Memory: False\n",
      "---\n",
      "Example 22:\n",
      "Query: Add dct:title to your knowledge base\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 23:\n",
      "Query: Note that geo:lat and geo:long are for coordinates\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 24:\n",
      "Query: I want you to remember that rdf:type connects instances to classes\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 25:\n",
      "Query: Remember vocab:hasTag is for tagging resources\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 26:\n",
      "Query: Store the fact that vcard:fn is for formatted names\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 27:\n",
      "Query: Keep in mind that sioc:content contains the text content\n",
      "Expected Tool: AddReflection\n",
      "Use Memory: False\n",
      "---\n",
      "Example 28:\n",
      "Query: What is dct:title used for?\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 29:\n",
      "Query: Tell me about geographic coordinates in RDF\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 30:\n",
      "Query: What do you recall about rdf:type?\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 31:\n",
      "Query: Can you remind me what vocab:hasTag is for?\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 32:\n",
      "Query: What is vcard:fn in the vCard ontology?\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n",
      "Example 33:\n",
      "Query: What was sioc:content used for?\n",
      "Expected Tool: RecallReflection\n",
      "Use Memory: True\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Display the development set\n",
    "for i, example in enumerate(devset):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Query: {example.q}\")\n",
    "    print(f\"Expected Tool: {example.exp_tool}\")\n",
    "    print(f\"Use Memory: {example.use_memory}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Metric Function\n",
    "\n",
    "We'll define a metric function to evaluate whether the agent is selecting the correct tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tool_match(pred, sample):\n",
    "    \"\"\"Check if the expected tool is in the trace.\"\"\"\n",
    "    return sample[\"exp_tool\"] in pred.get(\"trace\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Memory Planner Agent\n",
    "\n",
    "Now, we'll define a DSPy Module that integrates the memory tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MemoryPlanner(dspy.Module):\n",
    "    \"\"\"A DSPy module that selects the appropriate memory operation based on the query.\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_manager=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # If no graph manager is provided, create a mock one for testing\n",
    "        if graph_manager is None:\n",
    "            graph_manager = MagicMock(spec=GraphManager)\n",
    "        \n",
    "        # Create the reflection store\n",
    "        self.reflection_store = ReflectionStore(graph_manager)\n",
    "        \n",
    "        # Get all memory tools\n",
    "        self.add_reflection = get_tool_by_name(\"AddReflection\")()\n",
    "        self.recall_reflection = get_tool_by_name(\"RecallReflection\")()\n",
    "        self.reflection_prompt = get_tool_by_name(\"ReflectionPrompt\")()\n",
    "        \n",
    "        # Define the Chain of Thought for deciding which tool to use\n",
    "        self.decide_tool = dspy.ChainOfThought(\"query -> decision: str\")\n",
    "        \n",
    "    def forward(self, q):\n",
    "        \"\"\"Process a query and decide which memory tool to use.\"\"\"\n",
    "        trace = []\n",
    "        \n",
    "        # Use ChainOfThought to decide which tool to use\n",
    "        tool_decision = self.decide_tool(query=q)\n",
    "        \n",
    "        # Based on the decision, choose the appropriate tool\n",
    "        if \"add\" in tool_decision.decision.lower() or \"store\" in tool_decision.decision.lower() or \"remember\" in tool_decision.decision.lower():\n",
    "            note_id = self.add_reflection(text=q, tags=[\"user_query\"])\n",
    "            trace.append(\"AddReflection\")\n",
    "            response = f\"I've stored that information with ID: {note_id}\"\n",
    "            \n",
    "        elif \"recall\" in tool_decision.decision.lower() or \"retrieve\" in tool_decision.decision.lower() or \"what\" in tool_decision.decision.lower():\n",
    "            notes = self.recall_reflection(limit=3)\n",
    "            trace.append(\"RecallReflection\")\n",
    "            if notes:\n",
    "                notes_text = \"\\n\".join([f\"• {note.content['text']}\" for note in notes])\n",
    "                response = f\"Here's what I recall:\\n{notes_text}\"\n",
    "            else:\n",
    "                response = \"I don't have any relevant memories about that.\"\n",
    "                \n",
    "        elif \"prompt\" in tool_decision.decision.lower() or \"format\" in tool_decision.decision.lower() or \"inject\" in tool_decision.decision.lower():\n",
    "            formatted = self.reflection_prompt(limit=5)\n",
    "            trace.append(\"ReflectionPrompt\")\n",
    "            response = f\"I've prepared these notes for inclusion in the prompt:\\n{formatted}\"\n",
    "            \n",
    "        else:\n",
    "            response = \"I'm not sure how to process that as a memory operation.\"\n",
    "            \n",
    "        return {\"response\": response, \"trace\": trace, \"tool_decision\": tool_decision.decision}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Memory Planner with DSPy\n",
    "\n",
    "Now we'll train our memory planner using the DSPy BootstrapFewShot optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_memory_planner(devset, metric=tool_match, num_iterations=3, graph_manager=None):\n",
    "    \"\"\"Train the memory planner using DSPy's compilation framework.\"\"\"\n",
    "    # Create the base planner\n",
    "    planner = MemoryPlanner(graph_manager)\n",
    "    \n",
    "    # Set up the bootstrap trainer\n",
    "    trainer = BootstrapFewShot(trainset=devset, metric=metric)\n",
    "    \n",
    "    # Configure search space for optimization\n",
    "    search_space = {\n",
    "        \"RecallReflection.limit\": [3, 5, 10],\n",
    "        \"ReflectionPrompt.limit\": [3, 5, 10]\n",
    "    }\n",
    "    \n",
    "    # When using a real LLM, uncomment this line to run compilation\n",
    "    # optimized_planner = dspy.compile(planner, trainer, num_iterations=num_iterations, search_space=search_space)\n",
    "    \n",
    "    # For testing without a real LLM, we'll just return the unoptimized planner\n",
    "    optimized_planner = planner\n",
    "    \n",
    "    return optimized_planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_memory_planner_simba(\n",
    "    trainset,\n",
    "    metric=tool_match,\n",
    "    graph_manager=None,\n",
    "    max_steps: int = 20,\n",
    "    max_demos: int = 5,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Train the MemoryPlanner’s tool-selection policy using SIMBA.\"\"\"\n",
    "    planner = MemoryPlanner(graph_manager)\n",
    "    import dspy\n",
    "    simba = dspy.SIMBA(\n",
    "        metric=metric,\n",
    "        max_steps=max_steps,\n",
    "        max_demos=max_demos,\n",
    "    )\n",
    "    optimized = simba.compile(\n",
    "        student=planner,\n",
    "        trainset=trainset,\n",
    "        seed=seed,\n",
    "    )\n",
    "    return optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/13 18:42:47 INFO dspy.teleprompt.simba: Starting batch 1 of 20.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MockLM' object has no attribute 'kwargs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m     optimized_planner = train_memory_planner(devset, metric=tool_match, graph_manager=graph_manager)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     optimized_planner = \u001b[43mtrain_memory_planner_simba\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgraph_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgraph_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_demos\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Training complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_memory_planner_simba\u001b[39m\u001b[34m(trainset, metric, graph_manager, max_steps, max_demos, seed)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdspy\u001b[39;00m\n\u001b[32m     13\u001b[39m simba = dspy.SIMBA(\n\u001b[32m     14\u001b[39m     metric=metric,\n\u001b[32m     15\u001b[39m     max_steps=max_steps,\n\u001b[32m     16\u001b[39m     max_demos=max_demos,\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m optimized = \u001b[43msimba\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplanner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m optimized\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/git/LA3D/cogitarelink/cogitarelink-dspy/.venv/lib/python3.13/site-packages/dspy/teleprompt/simba.py:140\u001b[39m, in \u001b[36mSIMBA.compile\u001b[39m\u001b[34m(self, student, trainset, seed)\u001b[39m\n\u001b[32m    136\u001b[39m instance_idx += \u001b[38;5;28mself\u001b[39m.bsize\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# We'll generate (program, model) pairs for the trajectory sampling.\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Prepare distinct LMs (with different temperatures, etc.) from the baseline=programs[0].\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m models = \u001b[43mprepare_models_for_resampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprograms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m top_programs = top_k_plus_baseline(\u001b[38;5;28mself\u001b[39m.num_candidates)\n\u001b[32m    143\u001b[39m exec_pairs = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/git/LA3D/cogitarelink/cogitarelink-dspy/.venv/lib/python3.13/site-packages/dspy/teleprompt/simba_utils.py:16\u001b[39m, in \u001b[36mprepare_models_for_resampling\u001b[39m\u001b[34m(program, n)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare_models_for_resampling\u001b[39m(program: dspy.Module, n: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     15\u001b[39m     lm = program.get_lm() \u001b[38;5;129;01mor\u001b[39;00m dspy.settings.lm\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     temps = [\u001b[43mlm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m]] + [\u001b[32m0.5\u001b[39m + i * (\u001b[32m0.5\u001b[39m / n) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[32m     17\u001b[39m     temps = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mdict\u001b[39m.fromkeys(temps))[:n]\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [lm.copy(temperature=t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m temps]\n",
      "\u001b[31mAttributeError\u001b[39m: 'MockLM' object has no attribute 'kwargs'"
     ]
    }
   ],
   "source": [
    "# Train the memory planner\n",
    "from cogitarelink.core.graph import GraphManager\n",
    "graph_manager = GraphManager()\n",
    "if len(devset) < 32:\n",
    "    # fallback to BootstrapFewShot when dataset is too small\n",
    "    print(f\"Trainset too small ({len(devset)} < 32); using BootstrapFewShot instead\")\n",
    "    optimized_planner = train_memory_planner(devset, metric=tool_match, graph_manager=graph_manager)\n",
    "else:\n",
    "    optimized_planner = train_memory_planner_simba(\n",
    "        trainset=devset,\n",
    "        graph_manager=graph_manager,\n",
    "        max_steps=20,\n",
    "        max_demos=5,\n",
    "        seed=42,\n",
    "    )\n",
    "print(\"✅ Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load the Optimized Planner\n",
    "\n",
    "Now we'll save the optimized planner so it can be distributed with the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_optimized_planner(planner, path=\"../cogitarelink_dspy/optimized/memory_planner.pkl\"):\n",
    "    \"\"\"Save the optimized memory planner to disk.\"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Save the planner using pickle\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(planner, f)\n",
    "    \n",
    "    return path\n",
    "\n",
    "def load_optimized_planner(path=\"../cogitarelink_dspy/optimized/memory_planner.pkl\", graph_manager=None):\n",
    "    \"\"\"Load the optimized memory planner from disk.\"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Optimized planner not found at {path}\")\n",
    "    \n",
    "    # Load the planner using pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        planner = pickle.load(f)\n",
    "    \n",
    "    # If a graph manager is provided, update the planner's reflection store\n",
    "    if graph_manager is not None:\n",
    "        planner.reflection_store = ReflectionStore(graph_manager)\n",
    "    \n",
    "    return planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized planner\n",
    "saved_path = save_optimized_planner(optimized_planner)\n",
    "print(f\"Saved optimized planner to {saved_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Optimized Planner\n",
    "\n",
    "Let's test the optimized planner with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized planner\n",
    "loaded_planner = load_optimized_planner()\n",
    "\n",
    "# Test with a few examples\n",
    "test_queries = [\n",
    "    \"Remember that owl:sameAs is used for identity statements\",\n",
    "    \"What do you know about Wikidata properties?\",\n",
    "    \"Format the recent notes for inclusion in the prompt\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = loaded_planner(q=query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Selected Tool: {result['trace']}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with the Cogitarelink DSPy Agent\n",
    "\n",
    "Now we'll show how to integrate the optimized memory planner with the main Cogitarelink DSPy agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CogitarelinkAgent(dspy.Module):\n",
    "    \"\"\"A DSPy agent for Cogitarelink with integrated reflection memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_manager=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # If no graph manager is provided, create a mock one for testing\n",
    "        if graph_manager is None:\n",
    "            graph_manager = MagicMock(spec=GraphManager)\n",
    "        \n",
    "        # Load the optimized memory planner\n",
    "        try:\n",
    "            self.memory_planner = load_optimized_planner(graph_manager=graph_manager)\n",
    "        except FileNotFoundError:\n",
    "            # Fall back to unoptimized planner if optimized version not found\n",
    "            self.memory_planner = MemoryPlanner(graph_manager)\n",
    "        \n",
    "        # Main agent's Chain of Thought\n",
    "        self.main_cot = dspy.ChainOfThought(\"query, context -> response\")\n",
    "        \n",
    "        # Tool selection module\n",
    "        self.select_tool = dspy.ChainOfThought(\"query -> tool_name: str, is_memory: bool\")\n",
    "        \n",
    "    def forward(self, query):\n",
    "        \"\"\"Process a query using the appropriate tools and memory.\"\"\"\n",
    "        # Decide whether to use memory tools or other tools\n",
    "        tool_selection = self.select_tool(query=query)\n",
    "        \n",
    "        if tool_selection.is_memory:\n",
    "            # Use the memory planner for memory operations\n",
    "            memory_result = self.memory_planner(q=query)\n",
    "            return {\n",
    "                \"response\": memory_result[\"response\"],\n",
    "                \"tool_used\": memory_result[\"trace\"][0] if memory_result[\"trace\"] else \"None\",\n",
    "                \"is_memory_operation\": True\n",
    "            }\n",
    "        else:\n",
    "            # For non-memory operations, use other tools...\n",
    "            # Get recent memories as context\n",
    "            context = self.memory_planner.reflection_prompt(limit=3)\n",
    "            \n",
    "            # Use main Chain of Thought with memory context\n",
    "            result = self.main_cot(query=query, context=context)\n",
    "            \n",
    "            # Optionally store the interaction in memory\n",
    "            if \"important information\" in query.lower():\n",
    "                self.memory_planner.add_reflection(\n",
    "                    text=f\"User asked: {query} | Response: {result.response}\",\n",
    "                    tags=[\"interaction\"]\n",
    "                )\n",
    "            \n",
    "            return {\n",
    "                \"response\": result.response,\n",
    "                \"tool_used\": tool_selection.tool_name,\n",
    "                \"is_memory_operation\": False\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full agent\n",
    "agent = CogitarelinkAgent()\n",
    "\n",
    "# Test with different types of queries\n",
    "queries = [\n",
    "    \"Remember that rdfs:label is used for display names\",  # Memory operation\n",
    "    \"What is the capital of France?\",  # Regular query\n",
    "    \"What important information do you have about Wikidata properties?\"  # Query that uses memory context\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    result = agent(query=query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Tool Used: {result['tool_used']}\")\n",
    "    print(f\"Is Memory Operation: {result['is_memory_operation']}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Define a memory planner that integrates with DSPy\n",
    "2. Train and optimize the planner using DSPy's compilation framework\n",
    "3. Save and load the optimized planner for distribution\n",
    "4. Integrate the memory system with a full Cogitarelink agent\n",
    "\n",
    "This approach allows for declarative optimization of the memory system and ensures that agents can effectively utilize semantic memory for reflective learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
